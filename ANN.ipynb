{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Single LTU(linear threshold unit) network using perceptrons "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<em>The decision boundary of each output neuron is linear, so Perceptrons are incapable\n",
    "of learning complex patterns (just like Logistic Regression classifiers)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from sklearn.datasets import load_iris\n",
    "from sklearn.linear_model import Perceptron\n",
    "from warnings import filterwarnings\n",
    "\n",
    "filterwarnings(action='ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "iris = load_iris()\n",
    "X = iris.data[:, (2, 3)] # petal length, petal width\n",
    "y = (iris.target == 0).astype(np.int) # Iris Setosa?\n",
    "\n",
    "per_clf = Perceptron(random_state=42)\n",
    "per_clf.fit(X, y)\n",
    "y_pred = per_clf.predict([[2, 0.5]])\n",
    "y_pred"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Training an MLP with TensorFlow’s High-Level API"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<em>The following\n",
    "code trains a DNN for classification with two hidden layers (one with 300\n",
    "neurons, and the other with 100 neurons) and a softmax output layer with 10\n",
    "neurons:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "\n",
    "# (X_train, y_train), (X_test, y_test) = tf.keras.datasets.mnist.load_data()X_train = X_train.astype(np.float32).reshape(-1, 28*28) / 255.0\n",
    "X_test = X_test.astype(np.float32).reshape(-1, 28*28) / 255.0\n",
    "y_train = y_train.astype(np.int32)\n",
    "y_test = y_test.astype(np.int32)\n",
    "X_valid, X_train = X_train[:5000], X_train[5000:]\n",
    "y_valid, y_train = y_train[:5000], y_train[5000:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "feature_cols = [tf.feature_column.numeric_column(\"X\", shape=[28 * 28])]\n",
    "dnn_clf = tf.estimator.DNNClassifier(hidden_units=[300,100], n_classes=10,\n",
    "                                     feature_columns=feature_cols)\n",
    "\n",
    "input_fn = tf.estimator.inputs.numpy_input_fn(\n",
    "    x={\"X\": X_train}, y=y_train, num_epochs=40, batch_size=50, shuffle=True)\n",
    "# dnn_clf.train(input_fn=input_fn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_input_fn = tf.estimator.inputs.numpy_input_fn(\n",
    "    x={\"X\": X_test}, y=y_test, shuffle=False)\n",
    "eval_results = dnn_clf.evaluate(input_fn=test_input_fn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "eval_results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Using plain TensorFlow"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Construction phase"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf \n",
    "import numpy as np\n",
    "\n",
    "tf.reset_default_graph()\n",
    "\n",
    "n_inputs = 28*28 \n",
    "n_hidden1 = 300\n",
    "n_hidden2 = 100\n",
    "n_outputs = 10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Setting placeholders for X and y \n",
    "\n",
    "X = tf.placeholder(dtype=tf.float32, shape=(None, n_inputs), name='X')\n",
    "y = tf.placeholder(dtype=tf.int32, shape=(None), name='y')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "# function that we will use to create one layer at a time.\n",
    "\n",
    "def neuron_layer(X, n_neurons, name, activation=None):\n",
    "    with tf.name_scope(name):\n",
    "        n_inputs = int(X.get_shape()[1])\n",
    "        std_dev = 2/np.sqrt(n_inputs)\n",
    "        init = tf.random.truncated_normal(shape=(n_inputs, n_neurons), stddev=std_dev)\n",
    "        W = tf.Variable(init, name='weights')\n",
    "        b = tf.Variable(tf.zeros([n_neurons]), name='biases')\n",
    "        z = tf.matmul(X, W) + b \n",
    "        \n",
    "        if activation == 'relu':\n",
    "            return tf.nn.relu(z)\n",
    "        else:\n",
    "            return z"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create the DNN (Deep neural network)\n",
    "\n",
    "with tf.name_scope('dnn'):\n",
    "    hidden1 = neuron_layer(X, n_hidden1, 'hidden1', activation='relu')\n",
    "    hidden2 = neuron_layer(hidden1, n_hidden2, 'hidden2', activation='relu')\n",
    "    output  = neuron_layer(hidden2, n_outputs, 'outputs')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<em>TensorFlow’s fully_connected() function creates a fully connected layer, where all the inputs are connected to\n",
    "all the neurons in the layer. It takes care of creating the weights and biases variables,\n",
    "with the proper initialization strategy, and it uses the ReLU activation function by\n",
    "default"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Using the fully connected instead of the created construction function \n",
    "\n",
    "from tensorflow.contrib.layers import fully_connected\n",
    "\n",
    "with tf.name_scope('dnn'):\n",
    "    hidden1 = fully_connected(X, n_hidden1, scope=\"hidden1\")\n",
    "    hidden2 = fully_connected(hidden1, n_hidden2, scope=\"hidden2\")\n",
    "    logits = fully_connected(hidden2, n_outputs, scope=\"outputs\",activation_fn=None)\n",
    "    \n",
    "# logits are the output of the network before going through the softmax activation function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We are using cross-entropy as out loss function \n",
    "\n",
    "with tf.name_scope(\"loss\"):\n",
    "    xentropy = tf.nn.sparse_softmax_cross_entropy_with_logits(labels=y, logits=logits)\n",
    "    loss = tf.reduce_mean(xentropy, name=\"loss\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define a GradientDescentOptimizer that will tweak the model parameters to minimize the cost function.\n",
    "\n",
    "learning_rate = 0.01 \n",
    "\n",
    "with tf.name_scope('train'):\n",
    "    optimizer = tf.train.GradientDescentOptimizer(learning_rate=learning_rate)\n",
    "    training_op = optimizer.minimize(loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Specify how to evaluate the model.\n",
    "\n",
    "with tf.name_scope('eval'):\n",
    "    correct = tf.nn.in_top_k(logits, y, k=1)\n",
    "    accuracy = tf.reduce_mean(tf.cast(correct, tf.float32))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Writing log_directories for tensorboard \n",
    "\n",
    "from datetime import datetime\n",
    "\n",
    "now = datetime.utcnow().strftime(\"%Y%m%d%H%M%S\")\n",
    "root_logdir = \"tf_logs\"\n",
    "logdir = \"{}/run-{}/\".format(root_logdir, now)\n",
    "\n",
    "cost_summary = tf.summary.scalar('Cross_entropy_cost', tensor=loss)\n",
    "fileWriter = tf.summary.FileWriter(logdir=logdir, graph=tf.get_default_graph())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create a node to initialize all variables, and we will also create a Saver to save our trained model parameters to disk\n",
    "\n",
    "init = tf.global_variables_initializer()\n",
    "saver = tf.train.Saver()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Execution Phase"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prepairing the data\n",
    "\n",
    "from scipy.io import loadmat\n",
    "\n",
    "mnist_org = loadmat('mnist-original.mat')\n",
    "data = mnist_org['data'].T\n",
    "targets = mnist_org['label'].T\n",
    "X_train, X_test, y_train, y_test = data[:60000], data[60000:], targets[:60000], targets[60000:]\n",
    "\n",
    "shuffle_index = np.random.permutation(60000)\n",
    "X_train, y_train = X_train[shuffle_index], y_train[shuffle_index]\n",
    "y_train, y_test = y_train.ravel(), y_test.ravel()\n",
    "\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "scaler = StandardScaler()\n",
    "X_train = scaler.fit_transform(X_train)\n",
    "X_test = scaler.fit_transform(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_epochs = 20\n",
    "batch_size = 50\n",
    "n_batches = X_train.shape[0]//batch_size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Next batch function to quickly extract mini-batches \n",
    "\n",
    "def next_batch(size, X, y):\n",
    "    index = np.random.randint(X.shape[0], size=size)\n",
    "    X_batch = X[index]\n",
    "    y_batch = y[index]\n",
    "    return X_batch, y_batch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 Train accuracy: 0.9296833 Test accuracy: 0.9225\n",
      "1 Train accuracy: 0.9497167 Test accuracy: 0.9406\n",
      "2 Train accuracy: 0.9601 Test accuracy: 0.9492\n",
      "3 Train accuracy: 0.96635 Test accuracy: 0.9532\n",
      "4 Train accuracy: 0.97175 Test accuracy: 0.9583\n",
      "5 Train accuracy: 0.975 Test accuracy: 0.96\n",
      "6 Train accuracy: 0.97851664 Test accuracy: 0.9608\n",
      "7 Train accuracy: 0.9809333 Test accuracy: 0.964\n",
      "8 Train accuracy: 0.983 Test accuracy: 0.9649\n",
      "9 Train accuracy: 0.98503333 Test accuracy: 0.967\n",
      "10 Train accuracy: 0.98658335 Test accuracy: 0.968\n",
      "11 Train accuracy: 0.9881667 Test accuracy: 0.9687\n",
      "12 Train accuracy: 0.98898333 Test accuracy: 0.9691\n",
      "13 Train accuracy: 0.99053335 Test accuracy: 0.9707\n",
      "14 Train accuracy: 0.99121666 Test accuracy: 0.9702\n",
      "15 Train accuracy: 0.99256665 Test accuracy: 0.9713\n",
      "16 Train accuracy: 0.9931333 Test accuracy: 0.9721\n",
      "17 Train accuracy: 0.9942167 Test accuracy: 0.9712\n",
      "18 Train accuracy: 0.99475 Test accuracy: 0.9729\n",
      "19 Train accuracy: 0.99516666 Test accuracy: 0.9727\n"
     ]
    }
   ],
   "source": [
    "# Training the model \n",
    "\n",
    "with tf.Session() as sess:\n",
    "    init.run()\n",
    "    for epoch in range(n_epochs):\n",
    "        for iteration in range(n_batches):\n",
    "            X_batch, y_batch = next_batch(batch_size, X_train, y_train)\n",
    "            sess.run(training_op, feed_dict = {X: X_batch, y: y_batch})\n",
    "            \n",
    "            if iteration % 10 == 0:\n",
    "                summary_str = cost_summary.eval(feed_dict={X: X_batch, y: y_batch})\n",
    "                step = epoch * n_batches + iteration \n",
    "                fileWriter.add_summary(summary_str, step)\n",
    "                \n",
    "        acc_train = accuracy.eval(feed_dict = {X: X_train, y: y_train})\n",
    "        acc_test = accuracy.eval(feed_dict = {X: X_test, y: y_test})\n",
    "        print(epoch, \"Train accuracy:\", acc_train, \"Test accuracy:\", acc_test)\n",
    "        \n",
    "    save_path = saver.save(sess, \"./my_model_final.ckpt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fileWriter.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Using the Neural Network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Restoring parameters from ./my_model_final.ckpt\n"
     ]
    }
   ],
   "source": [
    "with tf.Session() as sess:\n",
    "    saver.restore(sess, \"./my_model_final.ckpt\")\n",
    "    Z = logits.eval(feed_dict={X: X_test})\n",
    "    y_pred = np.argmax(Z, axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<em>If you wanted to know all the estimated class probabilities, you would need to apply\n",
    "the softmax() function to the logits, but if you just want to predict a class, you can\n",
    "simply pick the class that has the highest logit value (using the argmax() function\n",
    "does the trick)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
