{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import tensorflow as tf \n",
    "import matplotlib.pyplot as plt \n",
    "import warnings\n",
    "\n",
    "%matplotlib inline\n",
    "warnings.filterwarnings(action='ignore')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### A simple RNN from scratch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "tf.reset_default_graph()\n",
    "\n",
    "n_inputs = 3\n",
    "n_neurons = 5\n",
    "\n",
    "X0 = tf.placeholder(tf.float32, shape=[None, n_inputs])\n",
    "X1 = tf.placeholder(tf.float32, shape=[None, n_inputs])\n",
    "\n",
    "Wx = tf.Variable(tf.random_normal(shape=[n_inputs, n_neurons], dtype=tf.float32))\n",
    "Wy = tf.Variable(tf.random_normal(shape=[n_neurons, n_neurons], dtype=tf.float32))\n",
    "b = tf.Variable(tf.zeros(shape=[1, n_neurons]), dtype=tf.float32)\n",
    "\n",
    "Y0 = tf.tanh(tf.matmul(X0, Wx) + b)\n",
    "Y1 = tf.tanh(tf.matmul(Y0, Wy) + tf.matmul(X1, Wx) + b)\n",
    "\n",
    "init = tf.global_variables_initializer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Mini-Batch:       instance 0, instance 1,instance 2,instance 3\n",
    "\n",
    "X0_batch = np.array([[0, 1, 2], [3, 4, 5], [6, 7, 8], [9, 0, 1]]) # t = 0\n",
    "X1_batch = np.array([[9, 8, 7], [0, 0, 0], [6, 5, 4], [3, 2, 1]]) # t = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "with tf.Session() as sess:\n",
    "    init.run()\n",
    "    Y0_val, Y1_val = sess.run([Y0, Y1], feed_dict={X0: X0_batch, X1: X1_batch})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[-0.3712633  -0.9422762   0.90898055  0.6768324  -0.9941537 ]\n",
      " [-0.9999776  -0.9999995   0.9757459   0.9993513  -0.99999785]\n",
      " [-1.         -1.          0.9936987   0.99999905 -1.        ]\n",
      " [-1.         -0.99999934 -0.40298423  0.9994849  -0.9994797 ]]\n"
     ]
    }
   ],
   "source": [
    "print(Y0_val) # output at t = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[-1.         -1.          0.5364571   0.9999998  -1.        ]\n",
      " [ 0.17842388  0.996354   -0.19095483 -0.6406073  -0.54699767]\n",
      " [-1.         -0.9999976  -0.32098052  0.99986297 -0.99996984]\n",
      " [-0.9987995  -0.8203782  -0.9952709   0.9821737  -0.9584852 ]]\n"
     ]
    }
   ],
   "source": [
    "print(Y1_val) # output at t = 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Static Unrolling Through Time"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The static_rnn() function creates an unrolled RNN network by chaining cells. The\n",
    "following code creates the exact same model as the previous one:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "tf.reset_default_graph()\n",
    "\n",
    "X0 = tf.placeholder(tf.float32, shape=[None, n_inputs])\n",
    "X1 = tf.placeholder(tf.float32, shape=[None, n_inputs])\n",
    "\n",
    "basic_cell = tf.keras.layers.SimpleRNNCell(units=n_neurons, activation='tanh')\n",
    "output_seqs, states = tf.nn.static_rnn(cell=basic_cell, inputs=[X0, X1], dtype=tf.float32)\n",
    "\n",
    "# When you are using\n",
    "# basic cells, the final state is simply equal to the last output.\n",
    "\n",
    "Y0, Y1 = output_seqs\n",
    "\n",
    "init = tf.global_variables_initializer()\n",
    "\n",
    "with tf.Session() as sess:\n",
    "    init.run()\n",
    "    Y0_val, Y1_val = sess.run([Y0, Y1], feed_dict={X0: X0_batch, X1: X1_batch})\n",
    "    final_state = states.eval(feed_dict={X0: X0_batch, X1: X1_batch})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 0.94276506, -0.99656993, -0.9995738 ,  1.        ,  0.99961364],\n",
       "       [-0.4096585 , -0.716821  ,  0.38663137,  0.8855186 ,  0.16710818],\n",
       "       [ 0.9494632 , -0.9994471 , -0.9846877 ,  0.99999994,  0.97973204],\n",
       "       [ 0.9914004 , -0.99097985, -0.65060073,  0.999642  , -0.50370914]],\n",
       "      dtype=float32)"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Y1_val"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 0.94276506, -0.99656993, -0.9995738 ,  1.        ,  0.99961364],\n",
       "       [-0.4096585 , -0.716821  ,  0.38663137,  0.8855186 ,  0.16710818],\n",
       "       [ 0.9494632 , -0.9994471 , -0.9846877 ,  0.99999994,  0.97973204],\n",
       "       [ 0.9914004 , -0.99097985, -0.65060073,  0.999642  , -0.50370914]],\n",
       "      dtype=float32)"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "final_state"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Packing Sequences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "tf.reset_default_graph()\n",
    "\n",
    "n_steps = 2\n",
    "n_inputs = 3\n",
    "n_neurons = 5\n",
    "\n",
    "X = tf.placeholder(tf.float32, shape=[None, n_steps, n_inputs])\n",
    "\n",
    "# Then we extract a Python list of tensors along the first dimension (i.e., one\n",
    "# tensor per time step)\n",
    "X_seqs = tf.unstack((tf.transpose(X, perm=[1, 0, 2])))  # Swap first dim and second dim, keep the last dim as it is \n",
    "basic_cell = tf.keras.layers.SimpleRNNCell(units=n_neurons, activation='tanh')\n",
    "output_seqs, states = tf.nn.static_rnn(basic_cell, inputs=X_seqs, dtype=tf.float32)\n",
    "\n",
    "outputs = tf.transpose(tf.stack(output_seqs), perm=[1, 0, 2])\n",
    "\n",
    "init = tf.global_variables_initializer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Now we can run the network by feeding it a single tensor that contains all the minibatch\n",
    "# sequences:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[[0, 1, 2],\n",
       "        [9, 8, 7]],\n",
       "\n",
       "       [[3, 4, 5],\n",
       "        [0, 0, 0]],\n",
       "\n",
       "       [[6, 7, 8],\n",
       "        [6, 5, 4]],\n",
       "\n",
       "       [[9, 0, 1],\n",
       "        [3, 2, 1]]])"
      ]
     },
     "execution_count": 95,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_batch = np.array([# t = 0 t = 1\n",
    "                    [[0, 1, 2], [9, 8, 7]], # instance 0\n",
    "                    [[3, 4, 5], [0, 0, 0]], # instance 1\n",
    "                    [[6, 7, 8], [6, 5, 4]], # instance 2\n",
    "                    [[9, 0, 1], [3, 2, 1]]]) # instance 3\n",
    "X_batch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "with tf.Session() as sess:\n",
    "    init.run()\n",
    "    outputs_val = outputs.eval(feed_dict={X: X_batch})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Dynamic Unrolling Through Time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "tf.reset_default_graph()\n",
    "\n",
    "n_steps = 2\n",
    "n_inputs = 3\n",
    "n_neurons = 5\n",
    "\n",
    "X = tf.placeholder(tf.float32, [None, n_steps, n_inputs])\n",
    "basic_cell = tf.keras.layers.SimpleRNNCell(units=n_neurons)\n",
    "outputs, states = tf.nn.dynamic_rnn(basic_cell, X, dtype=tf.float32)\n",
    "\n",
    "init = tf.global_variables_initializer()\n",
    "\n",
    "with tf.Session() as sess:\n",
    "    init.run()\n",
    "    outputs_val = outputs.eval(feed_dict={X: X_batch})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Handling Variable Length Input Sequences"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<em>So far we have used only fixed-size input sequences (all exactly two steps long). What\n",
    "if the input sequences have variable lengths (e.g., like sentences)? In this case you\n",
    "should set the sequence_length parameter when calling the dynamic_rnn() (or\n",
    "static_rnn()) function; it must be a 1D tensor indicating the length of the input\n",
    "sequence for each instance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [],
   "source": [
    "tf.reset_default_graph()\n",
    "\n",
    "n_steps = 2\n",
    "n_inputs = 3\n",
    "n_neurons = 5\n",
    "\n",
    "X = tf.placeholder(tf.float32, [None, n_steps, n_inputs])\n",
    "basic_cell = tf.keras.layers.SimpleRNNCell(units=n_neurons)\n",
    "\n",
    "seq_length = tf.placeholder(tf.int32, shape=[None])\n",
    "seq_length = tf.placeholder(tf.int32, [None])\n",
    "outputs, states = tf.nn.dynamic_rnn(basic_cell, X, dtype=tf.float32, sequence_length=seq_length)\n",
    "\n",
    "init = tf.global_variables_initializer()\n",
    "\n",
    "                     # step 0     step 1\n",
    "X_batch = np.array([[[0, 1, 2], [9, 8, 7]], # instance 0\n",
    "                    [[3, 4, 5], [0, 0, 0]], # instance 1 (padded with a zero vector)\n",
    "                    [[6, 7, 8], [6, 5, 4]], # instance 2\n",
    "                    [[9, 0, 1], [3, 2, 1]]]) # instance 3\n",
    "\n",
    "seq_length_batch = [2, 1, 2, 2]\n",
    "\n",
    "with tf.Session() as sess:\n",
    "    init.run()\n",
    "    outputs_val, states_val = sess.run([outputs, states], feed_dict={X: X_batch, seq_length: seq_length_batch})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[[-0.6682938  -0.98525363 -0.9424816   0.6381121   0.94908243]\n",
      "  [-0.99350595 -1.         -0.9715961  -0.99555904  1.        ]]\n",
      "\n",
      " [[-0.9256099  -0.99999744 -0.9912698  -0.2731056   0.99999714]\n",
      "  [ 0.          0.          0.          0.          0.        ]]\n",
      "\n",
      " [[-0.98509985 -1.         -0.9987025  -0.8656395   1.        ]\n",
      "  [-0.95136887 -0.9999959  -0.797005   -0.91468775  1.        ]]\n",
      "\n",
      " [[ 0.9542289   0.69114906  0.9999984  -0.99987644  0.9999656 ]\n",
      "  [ 0.80366445 -0.88931924  0.8793504  -0.97636855  0.9998091 ]]]\n"
     ]
    }
   ],
   "source": [
    "print(outputs_val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[-0.99350595 -1.         -0.9715961  -0.99555904  1.        ]\n",
      " [-0.9256099  -0.99999744 -0.9912698  -0.2731056   0.99999714]\n",
      " [-0.95136887 -0.9999959  -0.797005   -0.91468775  1.        ]\n",
      " [ 0.80366445 -0.88931924  0.8793504  -0.97636855  0.9998091 ]]\n"
     ]
    }
   ],
   "source": [
    "print(states_val)  # The states tensor contains the final state of each cell (excluding the zero vectors)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training a sequence classifer "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<em>\n",
    "- We will treat each image as a sequence of\n",
    " 28 rows of 28 pixels each (since each MNIST image is 28 × 28 pixels).<br>\n",
    "- We are creating a RNN with 150 neurons and we will unroll it 28 times steps since each training instance will be 28 inputs long\n",
    "- Each input will also contain 28 features "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<em>\n",
    "fully connected layer is connected to the states tensor,\n",
    "which contains only the final state of the RNN (i.e., the 28th output)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "metadata": {},
   "outputs": [],
   "source": [
    "tf.reset_default_graph()\n",
    "\n",
    "n_steps = 28 \n",
    "n_inputs = 28\n",
    "n_neurons = 150\n",
    "n_outputs = 10\n",
    "\n",
    "learning_rate = 0.001 \n",
    "\n",
    "X = tf.placeholder(tf.float32, shape=[None, n_steps, n_inputs])\n",
    "y = tf.placeholder(tf.int32, shape=[None]) # placeholder for the target classes\n",
    "\n",
    "basic_cell = tf.keras.layers.SimpleRNNCell(units=n_neurons)\n",
    "outputs, states = tf.nn.dynamic_rnn(basic_cell, X, dtype=tf.float32)\n",
    "logits = tf.layers.dense(inputs=states, units=n_outputs)\n",
    "\n",
    "xentropy = tf.nn.sparse_softmax_cross_entropy_with_logits(labels=y, logits=logits)\n",
    "loss = tf.reduce_mean(xentropy)\n",
    "\n",
    "optimizer = tf.train.AdamOptimizer(learning_rate=learning_rate)\n",
    "training_op = optimizer.minimize(loss)\n",
    "\n",
    "correct = tf.nn.in_top_k(logits, targets=y, k=1)\n",
    "accuracy = tf.reduce_mean(tf.cast(correct, dtype=tf.float32))\n",
    "\n",
    "init = tf.global_variables_initializer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Loading and prepairing the data\n",
    "\n",
    "from scipy.io import loadmat\n",
    "\n",
    "mnist_org = loadmat('mnist-original.mat')\n",
    "data = mnist_org['data'].T\n",
    "targets = mnist_org['label'].T\n",
    "X_train, X_test, y_train, y_test = data[:60000], data[60000:], targets[:60000], targets[60000:]\n",
    "\n",
    "shuffle_index = np.random.permutation(60000)\n",
    "X_train, y_train = X_train[shuffle_index], y_train[shuffle_index]\n",
    "y_train, y_test = y_train.ravel(), y_test.ravel()\n",
    "\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "scaler = StandardScaler()\n",
    "X_train = scaler.fit_transform(X_train)\n",
    "X_test = scaler.fit_transform(X_test)\n",
    "\n",
    "# Reshape the test data first \n",
    "X_test = X_test.reshape((-1, n_steps, n_inputs))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Next batch function to quickly extract mini-batches \n",
    "\n",
    "def next_batch(size, X, y):\n",
    "    index = np.random.randint(X.shape[0], size=size)\n",
    "    X_batch = X[index]\n",
    "    y_batch = y[index]\n",
    "    return X_batch, y_batch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 Train accuracy: 0.9266667 Test accuracy: 0.9269\n",
      "1 Train accuracy: 0.98 Test accuracy: 0.94\n",
      "2 Train accuracy: 0.94 Test accuracy: 0.9379\n",
      "3 Train accuracy: 0.98 Test accuracy: 0.9597\n",
      "4 Train accuracy: 0.9866667 Test accuracy: 0.9583\n",
      "5 Train accuracy: 0.9866667 Test accuracy: 0.9632\n",
      "6 Train accuracy: 0.97333336 Test accuracy: 0.9615\n",
      "7 Train accuracy: 0.9533333 Test accuracy: 0.9615\n",
      "8 Train accuracy: 0.98 Test accuracy: 0.9662\n",
      "9 Train accuracy: 0.9866667 Test accuracy: 0.9677\n"
     ]
    }
   ],
   "source": [
    "n_epochs = 10\n",
    "batch_size = 150 \n",
    "n_batches = len(X_train) // batch_size\n",
    "\n",
    "with tf.Session() as sess:\n",
    "    init.run()\n",
    "    for epoch in range(n_epochs):\n",
    "        for iteration in range(n_batches):\n",
    "            X_batch, y_batch = next_batch(batch_size, X_train, y_train)\n",
    "            X_batch = X_batch.reshape((-1, n_steps, n_inputs))\n",
    "            sess.run(training_op, feed_dict={X: X_batch, y: y_batch})\n",
    "        acc_train = accuracy.eval(feed_dict={X: X_batch, y: y_batch})\n",
    "        acc_test = accuracy.eval(feed_dict={X: X_test, y: y_test})\n",
    "        print(epoch, \"Train accuracy:\", acc_train, \"Test accuracy:\", acc_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training to Predict Time Series"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "t_min, t_max = 0, 30\n",
    "resolution = 0.1\n",
    "n_steps = 20\n",
    "\n",
    "def time_series(t):\n",
    "    return t * np.sin(t) / 3 + 2 * np.sin(t*5)\n",
    "\n",
    "def next_batch(batch_size, n_steps):\n",
    "    t0 = np.random.rand(batch_size, 1) * (t_max - t_min - n_steps * resolution)\n",
    "    Ts = t0 + np.arange(0., n_steps + 1) * resolution\n",
    "    ys = time_series(Ts)\n",
    "    return ys[:, :-1].reshape(-1, n_steps, 1), ys[:, 1:].reshape(-1, n_steps, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 4.75580159,  5.19908438],\n",
       "       [ 5.19908438,  6.05928172],\n",
       "       [ 6.05928172,  7.23891075],\n",
       "       [ 7.23891075,  8.54501591],\n",
       "       [ 8.54501591,  9.73531271],\n",
       "       [ 9.73531271, 10.57658639],\n",
       "       [10.57658639, 10.90105802],\n",
       "       [10.90105802, 10.64692655],\n",
       "       [10.64692655,  9.87316753],\n",
       "       [ 9.87316753,  8.74497027],\n",
       "       [ 8.74497027,  7.49338151],\n",
       "       [ 7.49338151,  6.35903719],\n",
       "       [ 6.35903719,  5.53375692],\n",
       "       [ 5.53375692,  5.11429758],\n",
       "       [ 5.11429758,  5.07958273],\n",
       "       [ 5.07958273,  5.29697516],\n",
       "       [ 5.29697516,  5.55604697],\n",
       "       [ 5.55604697,  5.62156717],\n",
       "       [ 5.62156717,  5.29272014],\n",
       "       [ 5.29272014,  4.45404068]])"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_batch, y_batch = next_batch(50, n_steps)\n",
    "np.c_[X_batch[44], y_batch[44]] ## Input and output sequence "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(50, 20, 1)"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_batch.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Using an OuputProjectionWrapper"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's create the RNN. It will contain 100 recurrent neurons and we will unroll it over 20 time steps since each training instance will be 20 inputs long. Each input will contain only one feature (the value at that time). The targets are also sequences of 20 inputs, each containing a single value:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "tf.reset_default_graph()\n",
    "\n",
    "n_steps = 20\n",
    "n_inputs = 1\n",
    "n_neurons = 100\n",
    "n_outputs = 1\n",
    "\n",
    "X = tf.placeholder(tf.float32, shape=[None, n_steps, n_inputs])\n",
    "y = tf.placeholder(tf.float32, shape=[None, n_steps, n_outputs])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- At each time step we now have an output vector of size 100. But what we actually\n",
    "want is a single output value at each time step. The simplest solution is to wrap the\n",
    "cell in an OutputProjectionWrapper\n",
    "\n",
    "- The OutputProjectionWrapper adds a fully connected layer of linear neurons (i.e., without\n",
    "any activation function) on top of each output (but it does not affect the cell state).\n",
    "\n",
    "- All these fully connected layers share the same (trainable) weights and bias terms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "cell = tf.contrib.rnn.OutputProjectionWrapper(\n",
    "    tf.nn.rnn_cell.BasicRNNCell(num_units=n_neurons, activation=tf.nn.relu),\n",
    "    output_size=n_outputs)\n",
    "\n",
    "outputs, states = tf.nn.dynamic_rnn(cell, inputs=X, dtype=tf.float32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "learning_rate = 0.001\n",
    "\n",
    "loss = tf.reduce_mean(tf.square(outputs - y))\n",
    "\n",
    "unexplained_error = tf.reduce_sum(tf.square(y - outputs))\n",
    "total_error = tf.reduce_sum(tf.square(y - tf.reduce_mean(y, axis=0)))\n",
    "R2 = 1. - tf.div(unexplained_error, total_error)\n",
    "\n",
    "optimizer = tf.train.AdamOptimizer(learning_rate=learning_rate)\n",
    "training_op = optimizer.minimize(loss)\n",
    "\n",
    "init = tf.global_variables_initializer()\n",
    "saver = tf.train.Saver()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 0 \tMSE: 18.77428 \tR2 -0.12749219\n",
      "Iteration 100 \tMSE: 0.7097012 \tR2 0.9601144\n",
      "Iteration 200 \tMSE: 0.31224054 \tR2 0.982063\n",
      "Iteration 300 \tMSE: 0.1098454 \tR2 0.99461997\n",
      "Iteration 400 \tMSE: 0.06952478 \tR2 0.9959823\n",
      "Iteration 500 \tMSE: 0.06293467 \tR2 0.9963066\n",
      "Iteration 600 \tMSE: 0.06294356 \tR2 0.9968455\n",
      "Iteration 700 \tMSE: 0.05105515 \tR2 0.9975703\n",
      "Iteration 800 \tMSE: 0.048389606 \tR2 0.99766624\n",
      "Iteration 900 \tMSE: 0.065213755 \tR2 0.9966278\n",
      "Iteration 1000 \tMSE: 0.053504776 \tR2 0.9972218\n",
      "Iteration 1100 \tMSE: 0.04769876 \tR2 0.9970911\n",
      "Iteration 1200 \tMSE: 0.044577383 \tR2 0.9972022\n",
      "Iteration 1300 \tMSE: 0.050732225 \tR2 0.9971172\n",
      "Iteration 1400 \tMSE: 0.038553927 \tR2 0.99788034\n"
     ]
    }
   ],
   "source": [
    "n_iterations = 1500\n",
    "batch_size = 50 \n",
    "\n",
    "with tf.Session() as sess:\n",
    "    init.run()\n",
    "    for iteration in range(n_iterations):\n",
    "        X_batch, y_batch = next_batch(batch_size, n_steps)\n",
    "        sess.run(training_op, feed_dict={X: X_batch, y: y_batch})\n",
    "        if iteration % 100 == 0:\n",
    "            mse = loss.eval(feed_dict={X: X_batch, y: y_batch})\n",
    "            r2 = R2.eval(feed_dict={X: X_batch, y: y_batch})\n",
    "            print('Iteration', iteration, '\\tMSE:', mse, '\\tR2', r2)\n",
    "    saver.save(sess, \"./my_time_series_model\") "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From C:\\ProgramData\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\training\\saver.py:1266: checkpoint_exists (from tensorflow.python.training.checkpoint_management) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use standard file APIs to check for files with this prefix.\n",
      "INFO:tensorflow:Restoring parameters from ./my_time_series_model\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 't_instance' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-16-24bba28797bc>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      2\u001b[0m     \u001b[0msaver\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mrestore\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0msess\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m\"./my_time_series_model\"\u001b[0m\u001b[1;33m)\u001b[0m   \u001b[1;31m# not shown\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 4\u001b[1;33m     \u001b[0mX_new\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtime_series\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0marray\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mt_instance\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m-\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mreshape\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m-\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mn_steps\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mn_inputs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      5\u001b[0m     \u001b[0my_pred\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0msess\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mrun\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0moutputs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfeed_dict\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;33m{\u001b[0m\u001b[0mX\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mX_new\u001b[0m\u001b[1;33m}\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mNameError\u001b[0m: name 't_instance' is not defined"
     ]
    }
   ],
   "source": [
    "with tf.Session() as sess:                          # not shown in the book\n",
    "    saver.restore(sess, \"./my_time_series_model\")   # not shown\n",
    "\n",
    "    X_new = time_series(np.array(t_instance[:-1].reshape(-1, n_steps, n_inputs)))\n",
    "    y_pred = sess.run(outputs, feed_dict={X: X_new})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
